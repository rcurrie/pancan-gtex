{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Optimize\n",
    "\n",
    "Train a neural network on PANCAN+GTEX gene expression to classify primary site and tumor/normal\n",
    "\n",
    "Optimize the model using [Hyperband](http://fastml.com/tuning-hyperparams-fast-with-hyperband/) after a subset of genes are identified based on the top Shapely values to determine of the same tumor/normal and disease classification can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG ON\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"DEBUG ON\") if os.environ.get(\"DEBUG\") else print(\"DEBUG OFF\")\n",
    "\n",
    "# Switch to pancan-gtex data directory so we can reference directly below\n",
    "os.chdir(os.path.expanduser(\"~/data/pancan-gtex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 17964 samples with 42326 features and 42 labels\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "X = pd.read_hdf(\"pancan_gtex.h5\", \"samples\")\n",
    "Y = pd.read_hdf(\"pancan_gtex.h5\", \"labels\")\n",
    "print(\"Loaded {} samples with {} features and {} labels\".format(X.shape[0], X.shape[1], Y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning to only include KEGG pathway genes\n",
      "Pruned expression to only include 5110 genes\n"
     ]
    }
   ],
   "source": [
    "# Prune X to only KEGG pathway genes\n",
    "with open(os.path.expanduser(\"~/data/msigdb/c2.cp.kegg.v6.2.symbols.gmt\")) as f:\n",
    "    subset_of_genes = list(set().union(*[line.strip().split(\"\\t\")[2:] for line in f.readlines()]))\n",
    "print(\"Pruning to only include KEGG pathway genes\")\n",
    "\n",
    "# Prune X to only Cosmic Cancer Genes\n",
    "# print(\"Pruning to only COSMIC genes\")\n",
    "# subset_of_genes = pd.read_table(os.path.expanduser(\"~/data/cosmic_260818.tsv\"))[\"Gene Symbol\"].values\n",
    "    \n",
    "pruned_X = X.drop(labels=(set(X.columns) - set(subset_of_genes)), axis=1)\n",
    "\n",
    "# Order must match dataframe so we can use this as labels for shap\n",
    "genes = list(pruned_X.columns.values)\n",
    "print(\"Pruned expression to only include\", len(genes), \"genes\")\n",
    "\n",
    "# Create a multi-label one-hot for tumor/normal and primary site\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "primary_site_encoder = LabelEncoder()\n",
    "Y[\"primary_site_value\"] = pd.Series(\n",
    "    primary_site_encoder.fit_transform(Y[\"_primary_site\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "tumor_normal_encoder = LabelEncoder()\n",
    "Y[\"tumor_normal_value\"] = pd.Series(\n",
    "    tumor_normal_encoder.fit_transform(Y[\"tumor_normal\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "from keras.utils import np_utils\n",
    "Y_onehot = np.append(\n",
    "    Y[\"tumor_normal_value\"].values.reshape(Y.shape[0],-1), \n",
    "    np_utils.to_categorical(Y[\"primary_site_value\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (14371, 700) Test: (3593, 700)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets strattified on primary site\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(X.values, Y.primary_site_value):\n",
    "    X_train = pruned_X.values[train_index]\n",
    "    X_test = pruned_X.values[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "    Y_onehot_train = Y_onehot[train_index]\n",
    "    Y_onehot_test = Y_onehot[test_index]\n",
    "    \n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Lets see how big each class is based on primary site\n",
    "# plt.hist(Y_train.primary_site_value.values, alpha=0.5, label='Train')\n",
    "# plt.hist(Y_test.primary_site_value.values, alpha=0.5, label='Test')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title(\"Primary site distribution between train and test sets\")\n",
    "# plt.show()\n",
    "\n",
    "# # Lets see how big each class is based tumor/normal\n",
    "# plt.hist(Y_train.tumor_normal_value.values, alpha=0.5, label='Train')\n",
    "# plt.hist(Y_test.tumor_normal_value.values, alpha=0.5, label='Test')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title(\"Tumor/normal distribution between train and test sets\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/mbernico/deep_learning_quick_reference/blob/master/chapter_6/mnist_hyperband_search.py\n",
    "import numpy as np\n",
    "import random\n",
    "from math import log, ceil\n",
    "from time import time, ctime\n",
    "\n",
    "class Hyperband:\n",
    "    def __init__(self, data, get_params_function, try_params_function, max_iter=81):\n",
    "        self.data = data\n",
    "        self.get_params = get_params_function\n",
    "        self.try_params = try_params_function\n",
    "\n",
    "        self.max_iter = max_iter  # maximum iterations per configuration\n",
    "        self.eta = 3  # defines configuration downsampling rate (default = 3)\n",
    "\n",
    "        self.logeta = lambda x: log(x) / log(self.eta)\n",
    "        self.s_max = int(self.logeta(self.max_iter))\n",
    "        self.B = (self.s_max + 1) * self.max_iter\n",
    "\n",
    "        self.results = []  # list of dicts\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_counter = -1\n",
    "\n",
    "    # can be called multiple times\n",
    "    def run(self, skip_last=0):\n",
    "\n",
    "        for s in list(reversed(range(self.s_max + 1)))[:1 if os.environ.get(\"DEBUG\") else -1]:\n",
    "\n",
    "            # initial number of configurations\n",
    "            n = int(ceil(self.B / self.max_iter / (s + 1) * self.eta ** s))\n",
    "\n",
    "            # initial number of iterations per config\n",
    "            r = self.max_iter * self.eta ** (-s)\n",
    "\n",
    "            # n random configurations\n",
    "            T = [self.get_params() for i in range(n)]\n",
    "\n",
    "            for i in list(range((s + 1) - int(skip_last)))[:1 if os.environ.get(\"DEBUG\") else -1]:  # changed from s + 1\n",
    "\n",
    "                # Run each of the n configs for <iterations>\n",
    "                # and keep best (n_configs / eta) configurations\n",
    "\n",
    "                n_configs = n * self.eta ** (-i)\n",
    "                n_iterations = r * self.eta ** (i)\n",
    "\n",
    "                print(\"\\n*** {} configurations x {:.1f} iterations each\".format(\n",
    "                    n_configs, n_iterations))\n",
    "\n",
    "                val_losses = []\n",
    "                early_stops = []\n",
    "\n",
    "                for t in T[:1 if os.environ.get(\"DEBUG\") else -1]:\n",
    "\n",
    "                    self.counter += 1\n",
    "                    print(\"\\n{} | {} | lowest loss so far: {:.4f} (run {})\\n\".format(\n",
    "                        self.counter, ctime(), self.best_loss, self.best_counter))\n",
    "\n",
    "                    start_time = time()\n",
    "\n",
    "                    result = self.try_params(self.data, n_iterations, t)\n",
    "\n",
    "                    assert (type(result) == dict)\n",
    "                    assert ('loss' in result)\n",
    "\n",
    "                    seconds = int(round(time() - start_time))\n",
    "                    print(\"\\n{} seconds.\".format(seconds))\n",
    "\n",
    "                    loss = result['loss']\n",
    "                    val_losses.append(loss)\n",
    "\n",
    "                    early_stop = result.get('early_stop', False)\n",
    "                    early_stops.append(early_stop)\n",
    "\n",
    "                    # keeping track of the best result so far (for display only)\n",
    "                    # could do it be checking results each time, but hey\n",
    "                    if loss < self.best_loss:\n",
    "                        self.best_loss = loss\n",
    "                        self.best_counter = self.counter\n",
    "\n",
    "                    result['counter'] = self.counter\n",
    "                    result['seconds'] = seconds\n",
    "                    result['params'] = t\n",
    "                    result['iterations'] = n_iterations\n",
    "                    result['num_model_params'] = result['model'].count_params()\n",
    "                    del result['model']\n",
    "\n",
    "                    self.results.append(result)\n",
    "\n",
    "                # select a number of best configurations for the next loop\n",
    "                # filter out early stops, if any\n",
    "                indices = np.argsort(val_losses)\n",
    "                T = [T[i] for i in indices if not early_stops[i]]\n",
    "                T = T[0:int(n_configs / self.eta)]\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 81 configurations x 1.0 iterations each\n",
      "\n",
      "1 | Sun Sep  9 23:32:57 2018 | lowest loss so far: inf (run -1)\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 700)               2800      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                44864     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 58,974\n",
      "Trainable params: 57,574\n",
      "Non-trainable params: 1,400\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "14371/14371 [==============================] - 2s 159us/step - loss: 0.2230\n",
      "\n",
      "4 seconds.\n",
      "Completed training.\n",
      "Top hyperparameter combinations:\n",
      "{'loss': 0.084738733744759806, 'counter': 1, 'seconds': 4, 'params': {'batch_size': 64, 'depth': 3, 'width': 64, 'penalty': 1e-07}, 'iterations': 1.0, 'num_model_params': 58974}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, BatchNormalization, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "def load_data():\n",
    "    return {\n",
    "        \"train_X\": X_train, \"train_y\": Y_onehot_train,\n",
    "        \"val_X\": X_train, \"val_y\": Y_onehot_train, \n",
    "        \"test_X\": X_test, \"test_y\": Y_onehot_test}\n",
    "\n",
    "def create_model(input_shape, output_shape, hyperparameters={\"width\": 64, \"penalty\": 1e-5}):\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "\n",
    "    x = BatchNormalization()(inputs)\n",
    "        \n",
    "    for i in range(hyperparameters[\"depth\"]):\n",
    "        x = Dense(hyperparameters[\"width\"], activity_regularizer=regularizers.l1(hyperparameters[\"penalty\"]), activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(output_shape, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"batch_size\": random.choice([64]),\n",
    "        \"depth\": random.choice([2, 3, 5]),\n",
    "        \"width\": random.choice([32, 64]),\n",
    "        \"penalty\": random.choice([0.01, 0.05])\n",
    "    }\n",
    "\n",
    "def try_params(data, num_iters, hyperparameters):\n",
    "    model = create_model(X_train.shape[1], Y_onehot_train.shape[1], hyperparameters)\n",
    "    if os.environ.get(\"DEBUG\"):\n",
    "        model.summary()\n",
    "    model.fit(x=data[\"train_X\"], y=data[\"train_y\"],\n",
    "              batch_size=hyperparameters[\"batch_size\"],\n",
    "              epochs=int(num_iters))\n",
    "    loss = model.evaluate(x=data[\"val_X\"], y=data[\"val_y\"], verbose=0)\n",
    "    return {\"loss\": loss, \"model\": model}\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "hyperband = Hyperband(data, get_hyperparameters, try_params)\n",
    "results = hyperband.run()\n",
    "\n",
    "print(\"Completed training.\")\n",
    "\n",
    "print(\"Top hyperparameter combinations:\")\n",
    "print(sorted(results, key=lambda r: r[\"loss\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save details to disk so we can backhaul from training machine\n",
    "with open(\"models/pancan_gtex.params.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"hyperparameters\": results,\n",
    "        \"tumor_normal\": tumor_normal_encoder.classes_.tolist(),\n",
    "        \"primary_site\": primary_site_encoder.classes_.tolist(),\n",
    "        \"genes\": genes,\n",
    "        \"train_indices\": train_index.tolist(),\n",
    "        \"test_indices\": test_index.tolist()}))\n",
    "\n",
    "# with open(\"models/pancan_gtex.model.json\", \"w\") as f:\n",
    "#     f.write(model.to_json())\n",
    "\n",
    "# # See https://github.com/h5py/h5py/issues/712\n",
    "# os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\" \n",
    "# model.save_weights(\"models/pancan_gtex.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
