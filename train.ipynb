{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Optimize\n",
    "\n",
    "Train a neural network on PANCAN+GTEX gene expression to classify primary site and tumor/normal\n",
    "\n",
    "Optimize the model using [Hyperband](http://fastml.com/tuning-hyperparams-fast-with-hyperband/) after a subset of genes are identified based on the top Shapely values to determine of the same tumor/normal and disease classification can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "DEBUG: ON\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)  # reproducibility\n",
    "\n",
    "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
    "\n",
    "# Simple syntatic sugar for debug vs. train parameters\n",
    "def debug(debug_param, no_debug_param):\n",
    "    return debug_param if os.environ.get(\"DEBUG\") == \"True\" else no_debug_param\n",
    "print(debug(\"DEBUG: ON\", \"DEBUG: OFF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Profile: prp Endpoint: https://s3.nautilus.optiputer.net Bucket: stuartlab\n"
     ]
    }
   ],
   "source": [
    "# Connect to S3 via boto3 so we can read and write\n",
    "import boto3\n",
    "\n",
    "bucket_name = \"stuartlab\"\n",
    "\n",
    "session = boto3.session.Session(profile_name=os.getenv(\"AWS_PROFILE\"))\n",
    "bucket = session.resource(\n",
    "    \"s3\", endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\")).Bucket(bucket_name)\n",
    "print(\"S3 Profile: {} Endpoint: {} Bucket: {}\".format(\n",
    "    os.getenv(\"AWS_PROFILE\"), os.getenv(\"AWS_S3_ENDPOINT\"), bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 17964 samples with 6974 features and 40 labels\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"pancan-gtex.h5\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    bucket.download_file(\"pancan-gtex/pancan-gtex.h5\", \"pancan-gtex.h5\")\n",
    "    \n",
    "print(\"Loading dataset...\")\n",
    "X = pd.read_hdf(\"pancan-gtex.h5\", \"samples\")\n",
    "Y = pd.read_hdf(\"pancan-gtex.h5\", \"labels\")\n",
    "print(\"Loaded {} samples with {} features and {} labels\".format(X.shape[0], X.shape[1], Y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-label one-hot for tumor/normal and primary site\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "primary_site_encoder = LabelEncoder()\n",
    "Y[\"primary_site_value\"] = pd.Series(\n",
    "    primary_site_encoder.fit_transform(Y[\"_primary_site\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "tumor_normal_encoder = LabelEncoder()\n",
    "Y[\"tumor_normal_value\"] = pd.Series(\n",
    "    tumor_normal_encoder.fit_transform(Y[\"tumor_normal\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "Y_onehot = np.append(\n",
    "    Y[\"tumor_normal_value\"].values.reshape(Y.shape[0],-1), \n",
    "    tf.keras.utils.to_categorical(Y[\"primary_site_value\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (14371, 6974) Test: (3593, 6974)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets strattified on primary site\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(X.values, Y.primary_site_value):\n",
    "    X_train = X.values[train_index]\n",
    "    X_test = X.values[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "    Y_onehot_train = Y_onehot[train_index]\n",
    "    Y_onehot_test = Y_onehot[test_index]\n",
    "    \n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lets see how big each class is based on primary site\n",
    "plt.hist(Y_train.primary_site_value.values, alpha=0.5, label='Train')\n",
    "plt.hist(Y_test.primary_site_value.values, alpha=0.5, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Primary site distribution between train and test sets\")\n",
    "plt.show()\n",
    "\n",
    "# Lets see how big each class is based tumor/normal\n",
    "plt.hist(Y_train.tumor_normal_value.values, alpha=0.5, label='Train')\n",
    "plt.hist(Y_test.tumor_normal_value.values, alpha=0.5, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Tumor/normal distribution between train and test sets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/mbernico/deep_learning_quick_reference/blob/master/chapter_6/mnist_hyperband_search.py\n",
    "import numpy as np\n",
    "import random\n",
    "from math import log, ceil\n",
    "from time import time, ctime\n",
    "\n",
    "class Hyperband:\n",
    "    def __init__(self, data, get_params_function, try_params_function, max_iter=81):\n",
    "        self.data = data\n",
    "        self.get_params = get_params_function\n",
    "        self.try_params = try_params_function\n",
    "\n",
    "        self.max_iter = max_iter  # maximum iterations per configuration\n",
    "        self.eta = 3  # defines configuration downsampling rate (default = 3)\n",
    "\n",
    "        self.logeta = lambda x: log(x) / log(self.eta)\n",
    "        self.s_max = int(self.logeta(self.max_iter))\n",
    "        self.B = (self.s_max + 1) * self.max_iter\n",
    "\n",
    "        self.results = []  # list of dicts\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_counter = -1\n",
    "\n",
    "    # can be called multiple times\n",
    "    def run(self, skip_last=0):\n",
    "\n",
    "        for s in list(reversed(range(self.s_max + 1)))[:1 if os.environ.get(\"DEBUG\") else -1]:\n",
    "\n",
    "            # initial number of configurations\n",
    "            n = int(ceil(self.B / self.max_iter / (s + 1) * self.eta ** s))\n",
    "\n",
    "            # initial number of iterations per config\n",
    "            r = self.max_iter * self.eta ** (-s)\n",
    "\n",
    "            # n random configurations\n",
    "            T = [self.get_params() for i in range(n)]\n",
    "\n",
    "            for i in list(range((s + 1) - int(skip_last)))[:1 if os.environ.get(\"DEBUG\") else -1]:  # changed from s + 1\n",
    "\n",
    "                # Run each of the n configs for <iterations>\n",
    "                # and keep best (n_configs / eta) configurations\n",
    "\n",
    "                n_configs = n * self.eta ** (-i)\n",
    "                n_iterations = r * self.eta ** (i)\n",
    "\n",
    "                print(\"\\n*** {} configurations x {:.1f} iterations each\".format(\n",
    "                    n_configs, n_iterations))\n",
    "\n",
    "                val_losses = []\n",
    "                early_stops = []\n",
    "\n",
    "                for t in T[:1 if os.environ.get(\"DEBUG\") else -1]:\n",
    "\n",
    "                    self.counter += 1\n",
    "                    print(\"\\n{} | {} | lowest loss so far: {:.4f} (run {})\\n\".format(\n",
    "                        self.counter, ctime(), self.best_loss, self.best_counter))\n",
    "\n",
    "                    start_time = time()\n",
    "\n",
    "                    result = self.try_params(self.data, n_iterations, t)\n",
    "\n",
    "                    assert (type(result) == dict)\n",
    "                    assert ('loss' in result)\n",
    "\n",
    "                    seconds = int(round(time() - start_time))\n",
    "                    print(\"\\n{} seconds.\".format(seconds))\n",
    "\n",
    "                    loss = result['loss']\n",
    "                    val_losses.append(loss)\n",
    "\n",
    "                    early_stop = result.get('early_stop', False)\n",
    "                    early_stops.append(early_stop)\n",
    "\n",
    "                    # keeping track of the best result so far (for display only)\n",
    "                    # could do it be checking results each time, but hey\n",
    "                    if loss < self.best_loss:\n",
    "                        self.best_loss = loss\n",
    "                        self.best_counter = self.counter\n",
    "\n",
    "                    result['counter'] = self.counter\n",
    "                    result['seconds'] = seconds\n",
    "                    result['params'] = t\n",
    "                    result['iterations'] = n_iterations\n",
    "                    result['num_model_params'] = result['model'].count_params()\n",
    "                    del result['model']\n",
    "\n",
    "                    self.results.append(result)\n",
    "\n",
    "                # select a number of best configurations for the next loop\n",
    "                # filter out early stops, if any\n",
    "                indices = np.argsort(val_losses)\n",
    "                T = [T[i] for i in indices if not early_stops[i]]\n",
    "                T = T[0:int(n_configs / self.eta)]\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 81 configurations x 1.0 iterations each\n",
      "\n",
      "1 | Sat Nov 24 02:37:03 2018 | lowest loss so far: inf (run -1)\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6974)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6974)              27896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                446400    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 481,446\n",
      "Trainable params: 467,498\n",
      "Non-trainable params: 13,948\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "14371/14371 [==============================] - 6s 447us/step - loss: 26.1046\n",
      "\n",
      "11 seconds.\n",
      "Completed training.\n",
      "Top hyperparameter combinations:\n",
      "{'num_model_params': 481446, 'params': {'penalty': 0.05, 'width': 64, 'depth': 2, 'batch_size': 64}, 'counter': 1, 'loss': 0.6167949912783812, 'iterations': 1.0, 'seconds': 11}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def load_data():\n",
    "    return {\n",
    "        \"train_X\": X_train, \"train_y\": Y_onehot_train,\n",
    "        \"val_X\": X_train, \"val_y\": Y_onehot_train, \n",
    "        \"test_X\": X_test, \"test_y\": Y_onehot_test}\n",
    "\n",
    "def create_model(input_shape, output_shape, hyperparameters={\"width\": 64, \"penalty\": 1e-5}):\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "\n",
    "    x = BatchNormalization()(inputs)\n",
    "        \n",
    "    for i in range(hyperparameters[\"depth\"]):\n",
    "        x = Dense(hyperparameters[\"width\"], activity_regularizer=regularizers.l1(hyperparameters[\"penalty\"]), activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(output_shape, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"batch_size\": random.choice([64]),\n",
    "        \"depth\": random.choice([2, 3, 5]),\n",
    "        \"width\": random.choice([32, 64]),\n",
    "        \"penalty\": random.choice([0.01, 0.05])\n",
    "    }\n",
    "\n",
    "def try_params(data, num_iters, hyperparameters):\n",
    "    model = create_model(X_train.shape[1], Y_onehot_train.shape[1], hyperparameters)\n",
    "    if os.environ.get(\"DEBUG\"):\n",
    "        model.summary()\n",
    "    model.fit(x=data[\"train_X\"], y=data[\"train_y\"],\n",
    "              batch_size=hyperparameters[\"batch_size\"],\n",
    "              epochs=int(num_iters))\n",
    "    loss = model.evaluate(x=data[\"val_X\"], y=data[\"val_y\"], verbose=0)\n",
    "    return {\"loss\": loss, \"model\": model}\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "hyperband = Hyperband(data, get_hyperparameters, try_params)\n",
    "results = hyperband.run()\n",
    "\n",
    "print(\"Completed training.\")\n",
    "\n",
    "print(\"Top hyperparameter combinations:\")\n",
    "print(sorted(results, key=lambda r: r[\"loss\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on best parameters...\n",
      "{'penalty': 0.05, 'width': 64, 'depth': 2, 'batch_size': 64}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 6974)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6974)              27896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                446400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 481,446\n",
      "Trainable params: 467,498\n",
      "Non-trainable params: 13,948\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "14371/14371 [==============================] - 8s 524us/step - loss: 26.0756\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "Loss: 11.450926780700684\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on best parameters...\")\n",
    "\n",
    "params = sorted(results, key=lambda r: r[\"loss\"])[0][\"params\"]\n",
    "\n",
    "print(params)\n",
    "\n",
    "model = create_model(X_train.shape[1], Y_onehot_train.shape[1], params)\n",
    "if os.environ.get(\"DEBUG\"):\n",
    "    model.summary()\n",
    "model.fit(X_train, Y_onehot_train,\n",
    "          batch_size=params[\"batch_size\"], epochs=1,\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=2, verbose=1)])\n",
    "\n",
    "loss = model.evaluate(X_test, Y_onehot_test, steps=1)\n",
    "print(\"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model to rcurrie/pancan-gtex/models/model.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model back to S3 under our username so we don't overwrite other's\n",
    "\n",
    "src = \"params.json\"\n",
    "with open(\"params.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"hyperparameters\": params,\n",
    "        \"tumor_normal\": tumor_normal_encoder.classes_.tolist(),\n",
    "        \"primary_site\": primary_site_encoder.classes_.tolist(),\n",
    "        \"train_indices\": train_index.tolist(),\n",
    "        \"test_indices\": test_index.tolist()}))\n",
    "\n",
    "dest = \"{}/{}/models/params.json\".format(os.environ[\"USER\"], \"pancan-gtex\")\n",
    "bucket.Object(dest).upload_file(src, ExtraArgs={\"ACL\":\"public-read\"})\n",
    "\n",
    "import tempfile\n",
    "src = \"/tmp/{}.h5\".format(next(tempfile._get_candidate_names()))\n",
    "model.save(src)\n",
    "\n",
    "dest = \"{}/{}/models/model.h5\".format(os.environ[\"USER\"], \"pancan-gtex\")\n",
    "bucket.Object(dest).upload_file(src, ExtraArgs={\"ACL\":\"public-read\"})\n",
    "os.remove(src)\n",
    "\n",
    "print(\"Saved trained model to {}\".format(dest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
