{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Optimize\n",
    "\n",
    "Train a neural network on PANCAN+GTEX gene expression to classify primary site and tumor/normal\n",
    "\n",
    "Optimize the model using [Hyperband](http://fastml.com/tuning-hyperparams-fast-with-hyperband/) after a subset of genes are identified based on the top Shapely values to determine of the same tumor/normal and disease classification can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "DEBUG: ON\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
    "\n",
    "# Switch to a scratch data directory so all paths are local\n",
    "os.makedirs(os.path.expanduser(\"~/data/pancan-gtex\"), exist_ok=True)\n",
    "os.chdir(os.path.expanduser(\"~/data/pancan-gtex\"))\n",
    "\n",
    "# Syntatic sugar for debug vs. train parameters\n",
    "def debug(debug_param, no_debug_param):\n",
    "    return debug_param if os.environ.get(\"DEBUG\") == \"True\" else no_debug_param\n",
    "print(debug(\"DEBUG: ON\", \"DEBUG: OFF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Profile: prp Endpoint: https://s3.nautilus.optiputer.net Bucket: stuartlab\n",
      "Dataset files checksums and :\n",
      "2018-11-23T17:26:45.350000+00:00 ef1c9c15b5a1fd836521f0759e57dfa7-120 pancan-gtex\n",
      "2019-01-22T17:45:49.577000+00:00 db47628dee79f558ad403c0a6cfe163d-2 pancan-gtex/ensemble-to-hugo.tsv\n",
      "2019-01-22T17:45:44.892000+00:00 9ef4694dcf33963943c2e9fc11925bf7-64 pancan-gtex/pancan-gtex-transcript.h5\n"
     ]
    }
   ],
   "source": [
    "# Setup S3 connection to download and upload data\n",
    "import boto3\n",
    "\n",
    "bucket_name = \"stuartlab\"\n",
    "session = boto3.session.Session(profile_name=os.getenv(\"AWS_PROFILE\"))\n",
    "bucket = session.resource(\"s3\", endpoint_url=os.getenv(\"AWS_S3_ENDPOINT\")).Bucket(bucket_name)\n",
    "print(\"S3 Profile: {} Endpoint: {} Bucket: {}\".format(\n",
    "    os.getenv(\"AWS_PROFILE\"), os.getenv(\"AWS_S3_ENDPOINT\"), bucket_name))\n",
    "print(\"Dataset files checksums and :\")\n",
    "for obj in bucket.objects.filter(Prefix=\"pancan-gtex\"):\n",
    "    print(obj.last_modified.isoformat(), obj.e_tag[1:-1], obj.key) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 17277 samples with 7564 features and 40 labels\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"pancan-gtex.h5\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    bucket.download_file(\"pancan-gtex/pancan-gtex-transcript.h5\", \"pancan-gtex-transcript.h5\")\n",
    "    \n",
    "print(\"Loading dataset...\")\n",
    "X = pd.read_hdf(\"pancan-gtex-transcript.h5\", \"samples\")\n",
    "Y = pd.read_hdf(\"pancan-gtex-transcript.h5\", \"labels\")\n",
    "print(\"Loaded {} samples with {} features and {} labels\".format(X.shape[0], X.shape[1], Y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27611.557"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[\"TCGA-ZT-A8OM-01\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-label one-hot for tumor/normal and primary site\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "primary_site_encoder = LabelEncoder()\n",
    "Y[\"primary_site_value\"] = pd.Series(\n",
    "    primary_site_encoder.fit_transform(Y[\"_primary_site\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "tumor_normal_encoder = LabelEncoder()\n",
    "Y[\"tumor_normal_value\"] = pd.Series(\n",
    "    tumor_normal_encoder.fit_transform(Y[\"tumor_normal\"]), index=Y.index, dtype='int32')\n",
    "\n",
    "Y_onehot = np.append(\n",
    "    Y[\"tumor_normal_value\"].values.reshape(Y.shape[0],-1), \n",
    "    tf.keras.utils.to_categorical(Y[\"primary_site_value\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13821, 7564) Test: (3456, 7564)\n",
      "Train: (13821, 7564) Val: (1728, 7564) Test: (1728, 7564)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validate and test sets strattified on primary site\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "\n",
    "# Split into train test\n",
    "for train_index, test_index in split.split(X.values, Y.primary_site_value):\n",
    "    X_train = X.values[train_index]\n",
    "    X_test = X.values[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "    Y_onehot_train = Y_onehot[train_index]\n",
    "    Y_onehot_test = Y_onehot[test_index]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# Split test into validate and test\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for val_index, test_index in split.split(X_test, Y_test.primary_site_value):\n",
    "    X_val = X_test[val_index]\n",
    "    X_test = X_test[test_index]\n",
    "    Y_val = Y_test.iloc[val_index]\n",
    "    Y_test = Y_test.iloc[test_index]\n",
    "    Y_onehot_val = Y_onehot_test[val_index]\n",
    "    Y_onehot_test = Y_onehot_test[test_index]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot only in debug as when training we'll be in python on k8s\n",
    "if os.environ.get(\"DEBUG\") == \"True\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Lets see how big each class is based on primary site\n",
    "    plt.hist(Y_train.primary_site_value.values, alpha=0.5, label='Train')\n",
    "    plt.hist(Y_val.primary_site_value.values, alpha=0.5, label='Val')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(Y_train.primary_site_value.values, alpha=0.5, label='Train')\n",
    "    plt.hist(Y_test.primary_site_value.values, alpha=0.5, label='Test')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(Y_train.tumor_normal_value.values, alpha=0.5, label='Train')\n",
    "    plt.hist(Y_val.tumor_normal_value.values, alpha=0.5, label='Val')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.hist(Y_train.tumor_normal_value.values, alpha=0.5, label='Train')\n",
    "    plt.hist(Y_test.tumor_normal_value.values, alpha=0.5, label='Test')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/PacktPublishing/Deep-Learning-Quick-Reference/blob/master/Chapter06/hyperband.py\n",
    "import random\n",
    "from math import log, ceil\n",
    "from time import time, ctime\n",
    "\n",
    "\n",
    "class Hyperband:\n",
    "    def __init__(self, data, get_params_function, try_params_function, max_iter=81, eta=3):\n",
    "        self.data = data\n",
    "        self.get_params = get_params_function\n",
    "        self.try_params = try_params_function\n",
    "\n",
    "        self.max_iter = debug(2, max_iter)  # maximum iterations per configuration\n",
    "        self.eta = debug(2, eta)  # defines configuration downsampling rate\n",
    "\n",
    "        self.logeta = lambda x: log(x) / log(self.eta)\n",
    "        self.s_max = int(self.logeta(self.max_iter))\n",
    "        self.B = (self.s_max + 1) * self.max_iter\n",
    "\n",
    "        self.results = []  # list of dicts\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_counter = -1\n",
    "\n",
    "    # can be called multiple times\n",
    "    def run(self, skip_last=0, dry_run=False):\n",
    "\n",
    "        for s in list(reversed(range(self.s_max + 1))):\n",
    "\n",
    "            # initial number of configurations\n",
    "            n = int(ceil(self.B / self.max_iter / (s + 1) * self.eta ** s))\n",
    "\n",
    "            # initial number of iterations per config\n",
    "            r = self.max_iter * self.eta ** (-s)\n",
    "\n",
    "            # n random configurations\n",
    "            T = [self.get_params() for i in range(n)]\n",
    "\n",
    "            for i in list(range((s + 1) - int(skip_last))):  # changed from s + 1\n",
    "\n",
    "                # Run each of the n configs for <iterations>\n",
    "                # and keep best (n_configs / eta) configurations\n",
    "\n",
    "                n_configs = n * self.eta ** (-i)\n",
    "                n_iterations = r * self.eta ** (i)\n",
    "\n",
    "                print(\"\\n*** {} configurations x {:.1f} iterations each\".format(\n",
    "                    n_configs, n_iterations))\n",
    "\n",
    "                val_losses = []\n",
    "                early_stops = []\n",
    "\n",
    "                for t in T:\n",
    "\n",
    "\n",
    "                    self.counter += 1\n",
    "                    print(\"\\n{} | {} | lowest loss so far: {:.4f} (run {})\\n\".format(\n",
    "                        self.counter, ctime(), self.best_loss, self.best_counter))\n",
    "\n",
    "                    start_time = time()\n",
    "\n",
    "                    if dry_run:\n",
    "                        result = {'loss': random(), 'log_loss': random(), 'auc': random()}\n",
    "                    else:\n",
    "                        result = self.try_params(self.data, n_iterations, t)\n",
    "\n",
    "                    assert (type(result) == dict)\n",
    "                    assert ('loss' in result)\n",
    "\n",
    "                    seconds = int(round(time() - start_time))\n",
    "                    print(\"\\n{} seconds.\".format(seconds))\n",
    "\n",
    "                    loss = result['loss']\n",
    "                    val_losses.append(loss)\n",
    "\n",
    "                    early_stop = result.get('early_stop', False)\n",
    "                    early_stops.append(early_stop)\n",
    "\n",
    "                    # keeping track of the best result so far (for display only)\n",
    "                    # could do it be checking results each time, but hey\n",
    "                    if loss < self.best_loss:\n",
    "                        self.best_loss = loss\n",
    "                        self.best_counter = self.counter\n",
    "\n",
    "                    result['counter'] = self.counter\n",
    "                    result['seconds'] = seconds\n",
    "                    result['params'] = t\n",
    "                    result['iterations'] = n_iterations\n",
    "                    del result['model']  # So we don't end up with hundreds of models taking memory\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "\n",
    "                    self.results.append(result)\n",
    "\n",
    "                # select a number of best configurations for the next loop\n",
    "                # filter out early stops, if any\n",
    "                indices = np.argsort(val_losses)\n",
    "                T = [T[i] for i in indices if not early_stops[i]]\n",
    "                T = T[0:int(n_configs / self.eta)]\n",
    "\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 2 configurations x 1.0 iterations each\n",
      "\n",
      "1 | Tue Jan 22 17:49:51 2019 | lowest loss so far: inf (run -1)\n",
      "\n",
      "Epoch 1/1\n",
      "13821/13821 [==============================] - 7s 530us/step - loss: 177.7231 - acc: 0.7821\n",
      "1728/1728 [==============================] - 1s 359us/step\n",
      "\n",
      "10 seconds.\n",
      "\n",
      "2 | Tue Jan 22 17:50:02 2019 | lowest loss so far: 2.4565 (run 1)\n",
      "\n",
      "Epoch 1/1\n",
      "13821/13821 [==============================] - 8s 554us/step - loss: 0.5640 - acc: 0.8059 5s - loss: 0.6794 -  - ETA: 2s - loss: 0.6165 - a\n",
      "1728/1728 [==============================] - 0s 283us/step\n",
      "\n",
      "10 seconds.\n",
      "\n",
      "*** 1.0 configurations x 2.0 iterations each\n",
      "\n",
      "3 | Tue Jan 22 17:50:12 2019 | lowest loss so far: 0.3065 (run 2)\n",
      "\n",
      "Epoch 1/2\n",
      "13821/13821 [==============================] - 6s 427us/step - loss: 0.6186 - acc: 0.7584\n",
      "Epoch 2/2\n",
      "13821/13821 [==============================] - 4s 288us/step - loss: 0.2809 - acc: 0.9509\n",
      "1728/1728 [==============================] - 0s 283us/step\n",
      "\n",
      "13 seconds.\n",
      "\n",
      "*** 2 configurations x 2.0 iterations each\n",
      "\n",
      "4 | Tue Jan 22 17:50:25 2019 | lowest loss so far: 0.1581 (run 3)\n",
      "\n",
      "Epoch 1/2\n",
      "13821/13821 [==============================] - 6s 414us/step - loss: 12.6230 - acc: 0.7373\n",
      "Epoch 2/2\n",
      "13821/13821 [==============================] - 5s 360us/step - loss: 0.9079 - acc: 0.9570\n",
      "1728/1728 [==============================] - 1s 322us/step\n",
      "\n",
      "13 seconds.\n",
      "\n",
      "5 | Tue Jan 22 17:50:39 2019 | lowest loss so far: 0.1581 (run 3)\n",
      "\n",
      "Epoch 1/2\n",
      "13821/13821 [==============================] - 9s 676us/step - loss: 0.5015 - acc: 0.9036\n",
      "Epoch 2/2\n",
      "13821/13821 [==============================] - 8s 595us/step - loss: 0.2257 - acc: 0.9565\n",
      "1728/1728 [==============================] - 1s 387us/step\n",
      "\n",
      "20 seconds.\n",
      "Completed training.\n",
      "Top parameter combinations:\n",
      "{'params': {'penalty': 1e-05, 'depth': 5, 'width': 32, 'batchsize': 128}, 'counter': 3, 'early_stop': False, 'acc': 0.9574652391451376, 'loss': 0.15811258444079646, 'seconds': 13, 'iterations': 2.0}\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    return {\n",
    "        \"train_X\": X_train, \"train_y\": Y_onehot_train,\n",
    "        \"val_X\": X_val, \"val_y\": Y_onehot_val, \n",
    "        \"test_X\": X_test, \"test_y\": Y_onehot_test}\n",
    "\n",
    "def create_model(input_shape, output_shape, params={\"depth\": 2, \"width\": 64, \"penalty\": 0.05}):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_shape,))\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "        \n",
    "    for i in range(params[\"depth\"]):\n",
    "        x = tf.keras.layers.Dense(params[\"width\"], activation=\"relu\",\n",
    "                           activity_regularizer=tf.keras.regularizers.l1(params[\"penalty\"]))(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_params():\n",
    "    return {\n",
    "        \"batchsize\": random.choice([64, 128, 256]),\n",
    "        \"depth\": random.choice([2, 3, 4, 5]),\n",
    "        \"width\": random.choice([32, 64, 128]),\n",
    "        \"penalty\": random.choice([1e-5, 1e-3, 1e-2]),\n",
    "    }\n",
    "\n",
    "def try_params(data, num_iters, params):\n",
    "    model = create_model(X_train.shape[1], Y_onehot_train.shape[1], params)\n",
    "#     if os.environ.get(\"DEBUG\"):\n",
    "#         model.summary()\n",
    "    \n",
    "    early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=2, verbose=1)\n",
    "    history = model.fit(x=data[\"train_X\"], y=data[\"train_y\"],\n",
    "              batch_size=params[\"batchsize\"],\n",
    "              epochs=int(num_iters),\n",
    "              callbacks = [early_stopping_monitor])\n",
    "    loss, acc = model.evaluate(x=data[\"val_X\"], y=data[\"val_y\"], verbose=1)\n",
    "    return {\"loss\": loss, \"acc\": acc, \"model\": model, \"early_stop\": early_stopping_monitor.stopped_epoch != 0}\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "hyperband = Hyperband(data, get_params, try_params)\n",
    "\n",
    "results = hyperband.run()\n",
    "\n",
    "# import cProfile\n",
    "# cProfile.run(\"results = hyperband.run()\", \"stats\")\n",
    "\n",
    "print(\"Completed training.\")\n",
    "\n",
    "print(\"Top parameter combinations:\")\n",
    "print(sorted(results, key=lambda r: r[\"loss\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on best parameters...\n",
      "{'penalty': 1e-05, 'depth': 5, 'width': 32, 'batchsize': 128}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7564)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7564)              30256     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                242080    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 34)                1122      \n",
      "=================================================================\n",
      "Total params: 277,682\n",
      "Trainable params: 262,554\n",
      "Non-trainable params: 15,128\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "13821/13821 [==============================] - 5s 330us/step - loss: 0.5772 - acc: 0.7971\n",
      "1/1 [==============================] - 0s 424ms/step\n",
      "Acc: 0.957000732421875 Loss: 0.9426659941673279\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on best parameters...\")\n",
    "\n",
    "params = sorted(results, key=lambda r: r[\"loss\"])[0][\"params\"]\n",
    "\n",
    "print(params)\n",
    "\n",
    "model = create_model(X_train.shape[1], Y_onehot_train.shape[1], params)\n",
    "if os.environ.get(\"DEBUG\"):\n",
    "    model.summary()\n",
    "model.fit(X_train, Y_onehot_train,\n",
    "          batch_size=params[\"batchsize\"], epochs=debug(1, 100),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=2, verbose=1)])\n",
    "\n",
    "loss, acc = model.evaluate(X_test, Y_onehot_test, steps=1)\n",
    "print(\"Acc: {} Loss: {}\".format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model files and checksums:\n",
      "2019-01-19T20:47:25.400000+00:00 411ee9f1b92a4a1fc057994a8afc44cc-2 rcurrie/pancan-gtex/models/model-debug.h5\n",
      "2019-01-22T17:51:07.719000+00:00 0f42a3fa4d850873c468129d310deb92 rcurrie/pancan-gtex/models/model-transcript-debug.h5\n",
      "2019-01-21T23:19:48.802000+00:00 d5af5da2206d7921746445fda622da75-8 rcurrie/pancan-gtex/models/model.h5\n",
      "2019-01-19T20:47:23.954000+00:00 345e77a0364204eed637aa58aa61c763 rcurrie/pancan-gtex/models/params-debug.json\n",
      "2019-01-22T17:51:07.192000+00:00 cfdd1ef40ddcd9623b34e5fefcc103dc rcurrie/pancan-gtex/models/params-transcript-debug.json\n",
      "2019-01-21T23:19:47.379000+00:00 dec23a80ae61ce3a1673d1fb0b22eb7b rcurrie/pancan-gtex/models/params.json\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and parameters to S3 for use in evaluate prefix with debug\n",
    "# so we don't clobber the fully trained model when modifying this notebook locally\n",
    "bucket.Object(\"{}/pancan-gtex/models/params-transcript{}.json\".format(os.environ[\"USER\"], debug(\"-debug\", \"\"))).put(\n",
    "    Body=json.dumps({\n",
    "        \"tumor_normal\": tumor_normal_encoder.classes_.tolist(),\n",
    "        \"primary_site\": primary_site_encoder.classes_.tolist(),\n",
    "        \"test_ids\": list(Y_test.index.values)}), \n",
    "    ACL=\"public-read\")\n",
    "\n",
    "import tempfile\n",
    "src = \"/tmp/{}.h5\".format(next(tempfile._get_candidate_names()))\n",
    "model.save(src)\n",
    "\n",
    "bucket.Object(\"{}/pancan-gtex/models/model-transcript{}.h5\".format(os.environ[\"USER\"], debug(\"-debug\", \"\"))).upload_file(\n",
    "    src, ExtraArgs={\"ACL\":\"public-read\"})\n",
    "\n",
    "print(\"Trained model files and checksums:\")\n",
    "for obj in bucket.objects.filter(Prefix=\"{}/pancan-gtex/models/\".format(os.environ[\"USER\"])):\n",
    "    print(obj.last_modified.isoformat(), obj.e_tag[1:-1], obj.key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
